{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EIqK0BYZF8I"
      },
      "source": [
        "# Introdução\n",
        "\n",
        "Este notebook demonstra a implementação do modelo Bag of Words para processamento de linguagem natural (NLP), seguindo o tutorial disponível no [Kaggle](https://www.kaggle.com/code/vipulgandhi/bag-of-words-model-for-beginners). Serão realizados testes com frases em inglês e português para avaliar a eficácia do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ3ufgNrnr-j"
      },
      "source": [
        "## Teste com frases em Inglês:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrntB_gInwqy"
      },
      "outputs": [],
      "source": [
        "famous_quotes = [\n",
        "    \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",  # Pride and Prejudice\n",
        "    \"It was the best of times, it was the worst of times.\",  # A Tale of Two Cities\n",
        "    \"The only thing we have to fear is fear itself.\",  # Franklin D. Roosevelt\n",
        "    \"May the Force be with you.\",  # Star Wars\n",
        "    \"I'm the king of the world!\",  # Titanic\n",
        "    \"To infinity and beyond!\",  # Toy Story\n",
        "    \"There's no place like home.\",  # The Wizard of Oz\n",
        "    \"I am your father.\",  # Star Wars\n",
        "    \"Elementary, my dear Watson.\",  # The Adventures of Sherlock Holmes\n",
        "    \"I think, therefore I am.\",  # Rene Descartes\n",
        "    \"I'm gonna make him an offer he can't refuse.\",  # The Godfather\n",
        "    \"Frankly, my dear, I don't give a damn.\",  # Gone with the Wind\n",
        "    \"Inconceivable!\",  # The Princess Bride\n",
        "    \"You're gonna need a bigger boat.\",  # Jaws\n",
        "    \"I see dead people.\",  # The Sixth Sense\n",
        "    \"Why so serious?\",  # The Dark Knight\n",
        "    \"Houston, we have a problem.\",  # Apollo 13\n",
        "    \"Just keep swimming.\",  # Finding Nemo\n",
        "    \"Winter is coming.\",  # Game of Thrones\n",
        "    \"You can't handle the truth!\",  # A Few Good Men\n",
        "    \"My precious.\",  # The Lord of the Rings\n",
        "    \"What we've got here is failure to communicate.\",  # Cool Hand Luke\n",
        "    \"You talking to me?\",  # Taxi Driver\n",
        "    \"After all this time? Always.\",  # Harry Potter and the Deathly Hallows\n",
        "    \"Life is like a box of chocolates, you never know what you're gonna get.\"  # Forrest Gump\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZDN5UpouSN"
      },
      "source": [
        "O código abaixo cria uma lista com todas as palavras utilizadas na lista oferecida (famous_quotes), de modo a não repetir nenhuma palavra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRNLSNE6n4yx",
        "outputId": "d6466e25-f7dd-437a-fa32-61595b4a0b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['acknowledged', 'after', 'all', 'always', 'am', 'an', 'and', 'be', 'best', 'beyond', 'bigger', 'boat', 'box', 'can', 'chocolates', 'coming', 'communicate', 'damn', 'dead', 'dear', 'don', 'elementary', 'failure', 'father', 'fear', 'force', 'fortune', 'frankly', 'get', 'give', 'gonna', 'good', 'got', 'handle', 'have', 'he', 'here', 'him', 'home', 'houston', 'in', 'inconceivable', 'infinity', 'is', 'it', 'itself', 'just', 'keep', 'king', 'know', 'life', 'like', 'make', 'man', 'may', 'me', 'must', 'my', 'need', 'never', 'no', 'of', 'offer', 'only', 'people', 'place', 'possession', 'precious', 'problem', 're', 'refuse', 'see', 'serious', 'single', 'so', 'swimming', 'talking', 'that', 'the', 'there', 'therefore', 'thing', 'think', 'this', 'time', 'times', 'to', 'truth', 'universally', 've', 'want', 'was', 'watson', 'we', 'what', 'why', 'wife', 'winter', 'with', 'world', 'worst', 'you', 'your']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenizar e construir vocabulário\n",
        "vectorizer.fit(famous_quotes)\n",
        "\n",
        "# coloca em ordem\n",
        "sorted_list = sorted(vectorizer.vocabulary_)\n",
        "print(sorted_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eicK8wf5Tl0E"
      },
      "source": [
        "O código abaixo vetoriza cada uma das frases. Analisa-se a partir do tamanho (shape) que são 25 frases, que foram os imputs colocados, e cada umas das linhas do vetor tem 103 colunas, que representam a quantidade de palavras compostas no vetor. Cada linha do vetor é uma frase, e se aquela frase tem aquela palavra, ela será colocada a quantidade de vezes que a palavra aparece. Confira abaixo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoBklAV6pEi1",
        "outputId": "340a04c3-d1e9-4329-dd36-239ed8f2ed53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25, 103)\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [0 0 0 ... 0 2 0]]\n"
          ]
        }
      ],
      "source": [
        "# codificar documento\n",
        "vector = vectorizer.transform(famous_quotes)\n",
        "# resumir vetor codificado\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6C1f9bFVHaK",
        "outputId": "c9e8b84a-d6cb-4cba-f81a-cc7fd9800eff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYflRNprVWW6"
      },
      "source": [
        "Nota-se que o vetor é muito grande, uma forma de diminuílo seria tirando as stop-words. Que são palavras que aparecem no texto mas são insignificantes para o processamento de linguagem, e sua presença pode impactar o modelo de processamento, como conectores. Segue o resultado tirando as stop-words, o que consequentemente diminuem o vetor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-5I5SDbV69z",
        "outputId": "9a272dfa-fe11-4842-a79e-753813392fb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['truth universally acknowledged , single man possession good fortune , must want wife .',\n",
              " 'best times , worst times .',\n",
              " 'thing fear fear .',\n",
              " 'May Force .',\n",
              " \"'m king world !\",\n",
              " 'infinity beyond !',\n",
              " \"'s place like home .\",\n",
              " 'father .',\n",
              " 'Elementary , dear Watson .',\n",
              " 'think , therefore .',\n",
              " \"'m gon na make offer ca n't refuse .\",\n",
              " \"Frankly , dear , n't give damn .\",\n",
              " 'Inconceivable !',\n",
              " \"'re gon na need bigger boat .\",\n",
              " 'see dead people .',\n",
              " 'serious ?',\n",
              " 'Houston , problem .',\n",
              " 'keep swimming .',\n",
              " 'Winter coming .',\n",
              " \"ca n't handle truth !\",\n",
              " 'precious .',\n",
              " \"'ve got failure communicate .\",\n",
              " 'talking ?',\n",
              " 'time ? Always .',\n",
              " \"Life like box chocolates , never know 're gon na get .\"]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Função para remover stopwords de uma frase\n",
        "def remove_stopwords(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    filtered_sentence = [word for word in words if not word.lower() in stop_words]\n",
        "    return \" \".join(filtered_sentence)\n",
        "\n",
        "# Aplicando a função em todas as frases\n",
        "filtered_quotes = [remove_stopwords(quote) for quote in famous_quotes]\n",
        "filtered_quotes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dykStn-aW1Gi"
      },
      "source": [
        "Agora que temos uma lista com todas as palavras que aparecem nas frases, tirando as stop-words, seguimos o processo de BAg-of-Words semelhante da anterior:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3sRmJ8QYLMz",
        "outputId": "23a88185-30cd-47af-f29f-ea7f72628bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['acknowledged', 'always', 'best', 'beyond', 'bigger', 'boat', 'box', 'ca', 'chocolates', 'coming', 'communicate', 'damn', 'dead', 'dear', 'elementary', 'failure', 'father', 'fear', 'force', 'fortune', 'frankly', 'get', 'give', 'gon', 'good', 'got', 'handle', 'home', 'houston', 'inconceivable', 'infinity', 'keep', 'king', 'know', 'life', 'like', 'make', 'man', 'may', 'must', 'na', 'need', 'never', 'offer', 'people', 'place', 'possession', 'precious', 'problem', 're', 'refuse', 'see', 'serious', 'single', 'swimming', 'talking', 'therefore', 'thing', 'think', 'time', 'times', 'truth', 'universally', 've', 'want', 'watson', 'wife', 'winter', 'world', 'worst']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenizar e construir vocabulário\n",
        "vectorizer.fit(filtered_quotes)\n",
        "\n",
        "# coloca em ordem\n",
        "sorted_list = sorted(vectorizer.vocabulary_)\n",
        "print(sorted_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_KO4pVXXAud",
        "outputId": "b4f23ce2-8700-4639-d957-adc498b3d235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25, 70)\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# codificar documento\n",
        "vector = vectorizer.transform(filtered_quotes)\n",
        "# resumir vetor codificado\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE5YCxazYP7n"
      },
      "source": [
        "Veja que o tamanho do vetor diminuiu de 103 valores para 70 após a filtragem de Stop-words. Garantindo uma maior eficiência, tanto na economia de espaço, quanto na qualidade do modelo de treinamento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrKXKxETYuU3"
      },
      "source": [
        "## Teste com palavras em Português:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLkzOUJTYPte"
      },
      "outputs": [],
      "source": [
        "frases_famosas_pt = [\n",
        "    \"É uma verdade universalmente reconhecida que um homem solteiro, possuidor de uma boa fortuna, deve estar em busca de uma esposa.\",  # Orgulho e Preconceito\n",
        "    \"Era o melhor dos tempos, era o pior dos tempos.\",  # Um Conto de Duas Cidades\n",
        "    \"A única coisa que devemos temer é o próprio medo.\",  # Franklin D. Roosevelt\n",
        "    \"Que a Força esteja com você.\",  # Star Wars\n",
        "    \"Eu sou o rei do mundo!\",  # Titanic\n",
        "    \"Ao infinito e além!\",  # Toy Story\n",
        "    \"Não há lugar como nosso lar.\",  # O Mágico de Oz\n",
        "    \"Eu sou seu pai.\",  # Star Wars\n",
        "    \"Elementar, meu caro Watson.\",  # As Aventuras de Sherlock Holmes\n",
        "    \"Penso, logo existo.\",  # René Descartes\n",
        "    \"Vou fazer-lhe uma oferta que ele não poderá recusar.\",  # O Poderoso Chefão\n",
        "    \"Francamente, minha querida, eu não dou a mínima.\",  # E o Vento Levou\n",
        "    \"Inconcebível!\",  # A Princesa Prometida\n",
        "    \"Você vai precisar de um barco maior.\",  # Tubarão\n",
        "    \"Eu vejo pessoas mortas.\",  # O Sexto Sentido\n",
        "    \"Por que tão sério?\",  # O Cavaleiro das Trevas\n",
        "    \"Houston, temos um problema.\",  # Apollo 13\n",
        "    \"Continue nadando.\",  # Procurando Nemo\n",
        "    \"O inverno está chegando.\",  # Game of Thrones\n",
        "    \"Você não pode lidar com a verdade!\",  # Questão de Honra\n",
        "    \"Meu precioso.\",  # O Senhor dos Anéis\n",
        "    \"O que temos aqui é uma falha de comunicação.\",  # Rebeldia Indomável\n",
        "    \"Você está falando comigo?\",  # Taxi Driver\n",
        "    \"Depois de todo esse tempo? Sempre.\",  # Harry Potter e as Relíquias da Morte\n",
        "    \"A vida é como uma caixa de chocolates, você nunca sabe o que vai encontrar.\"  # Forrest Gump\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3xNRjetZuuL"
      },
      "source": [
        "Seguindo o mesmo processo que o anterior, temos a seguinte solução"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00pmJWytZjtC",
        "outputId": "67ff544a-29c6-4a4d-aa49-58f5446d2225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['além', 'ao', 'aqui', 'barco', 'boa', 'busca', 'caixa', 'caro', 'chegando', 'chocolates', 'coisa', 'com', 'comigo', 'como', 'comunicação', 'continue', 'de', 'depois', 'deve', 'devemos', 'do', 'dos', 'dou', 'ele', 'elementar', 'em', 'encontrar', 'era', 'esposa', 'esse', 'estar', 'esteja', 'está', 'eu', 'existo', 'falando', 'falha', 'fazer', 'fortuna', 'força', 'francamente', 'homem', 'houston', 'há', 'inconcebível', 'infinito', 'inverno', 'lar', 'lhe', 'lidar', 'logo', 'lugar', 'maior', 'medo', 'melhor', 'meu', 'minha', 'mortas', 'mundo', 'mínima', 'nadando', 'nosso', 'nunca', 'não', 'oferta', 'pai', 'penso', 'pessoas', 'pior', 'pode', 'poderá', 'por', 'possuidor', 'precioso', 'precisar', 'problema', 'próprio', 'que', 'querida', 'reconhecida', 'recusar', 'rei', 'sabe', 'sempre', 'seu', 'solteiro', 'sou', 'sério', 'temer', 'temos', 'tempo', 'tempos', 'todo', 'tão', 'um', 'uma', 'universalmente', 'vai', 'vejo', 'verdade', 'vida', 'você', 'vou', 'watson', 'única']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenizar e construir vocabulário\n",
        "vectorizer.fit(frases_famosas_pt)\n",
        "\n",
        "# coloca em ordem\n",
        "sorted_list = sorted(vectorizer.vocabulary_)\n",
        "print(sorted_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3plAjrzoZpPi",
        "outputId": "b5007196-f750-404b-dfe7-6b9cf1ce1d58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25, 105)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# codificar documento\n",
        "vector = vectorizer.transform(frases_famosas_pt)\n",
        "# resumir vetor codificado\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-VOqI1QZzqf"
      },
      "source": [
        "Agora vamos tentar retirando as stop-words, que novamente vai gerar uma diminuição do vetor de palavras ggarantindo eficiência para a solução final:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASR-ILchZ4Jo",
        "outputId": "2030a539-c05e-47a3-ae0e-d1bde8200e3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['verdade universalmente reconhecida homem solteiro , possuidor boa fortuna , deve busca esposa .',\n",
              " 'melhor tempos , pior tempos .',\n",
              " 'única coisa devemos temer próprio medo .',\n",
              " 'Força .',\n",
              " 'rei mundo !',\n",
              " 'infinito além !',\n",
              " 'lugar lar .',\n",
              " 'pai .',\n",
              " 'Elementar , caro Watson .',\n",
              " 'Penso , logo existo .',\n",
              " 'Vou fazer-lhe oferta poderá recusar .',\n",
              " 'Francamente , querida , dou mínima .',\n",
              " 'Inconcebível !',\n",
              " 'vai precisar barco maior .',\n",
              " 'vejo pessoas mortas .',\n",
              " 'tão sério ?',\n",
              " 'Houston , problema .',\n",
              " 'Continue nadando .',\n",
              " 'inverno chegando .',\n",
              " 'pode lidar verdade !',\n",
              " 'precioso .',\n",
              " 'aqui falha comunicação .',\n",
              " 'falando comigo ?',\n",
              " 'todo tempo ? Sempre .',\n",
              " 'vida caixa chocolates , nunca sabe vai encontrar .']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "# Função para remover stopwords de uma frase\n",
        "def remove_stopwords(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    filtered_sentence = [word for word in words if not word.lower() in stop_words]\n",
        "    return \" \".join(filtered_sentence)\n",
        "\n",
        "# Aplicando a função em todas as frases\n",
        "frases_famosas_pt_sem_stopwords = [remove_stopwords(frase) for frase in frases_famosas_pt]\n",
        "frases_famosas_pt_sem_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0rdBIKpage6",
        "outputId": "f01d443b-4489-490f-bb5a-5ac1551b6ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['além', 'aqui', 'barco', 'boa', 'busca', 'caixa', 'caro', 'chegando', 'chocolates', 'coisa', 'comigo', 'comunicação', 'continue', 'deve', 'devemos', 'dou', 'elementar', 'encontrar', 'esposa', 'existo', 'falando', 'falha', 'fazer', 'fortuna', 'força', 'francamente', 'homem', 'houston', 'inconcebível', 'infinito', 'inverno', 'lar', 'lhe', 'lidar', 'logo', 'lugar', 'maior', 'medo', 'melhor', 'mortas', 'mundo', 'mínima', 'nadando', 'nunca', 'oferta', 'pai', 'penso', 'pessoas', 'pior', 'pode', 'poderá', 'possuidor', 'precioso', 'precisar', 'problema', 'próprio', 'querida', 'reconhecida', 'recusar', 'rei', 'sabe', 'sempre', 'solteiro', 'sério', 'temer', 'tempo', 'tempos', 'todo', 'tão', 'universalmente', 'vai', 'vejo', 'verdade', 'vida', 'vou', 'watson', 'única']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(frases_famosas_pt_sem_stopwords)\n",
        "\n",
        "# summarize\n",
        "sorted_list = sorted(vectorizer.vocabulary_)\n",
        "print(sorted_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_hsDOIHamAU",
        "outputId": "5541eae9-128f-4be1-b64b-0ea331227c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25, 77)\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# codificar documento\n",
        "vector = vectorizer.transform(frases_famosas_pt_sem_stopwords)\n",
        "# resumir vetor codificado\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
